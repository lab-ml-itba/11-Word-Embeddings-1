{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Word Embeddings\n",
    "\n",
    "Sources:\n",
    "https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/\n",
    "http://sebastianruder.com/word-embeddings-1/index.html\n",
    "\n",
    "\n",
    "## A Brief of History\n",
    "Extracted from https://www.gavagai.se/blog/2015/09/30/a-brief-history-of-word-embeddings/\n",
    "\n",
    "Word embeddings are a epresentational basis for downstream NLP tasks like text classification, document clustering, part of speech tagging, named entity recognition, sentiment analysis, and so on.  \n",
    "\n",
    "Word embedding seems to be the dominating term at the moment, I often prefer the term distributional semantic model (since the underlying semantic theory is called [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)).  \n",
    "\n",
    "There are also many other alternative terms in use, from the very general distributed representation to the more specific semantic vector space or simply word space.  \n",
    "\n",
    "Methods for using automatically generated contextual features were developed more or less simultaneously around 1990 in several different research areas. One of the most influential early models was Latent Semantic Analysis/Indexing (LSA/LSI), developed in the context of information retrieval, and the precursor of today’s topic models.  \n",
    "\n",
    "The most well-known of these are probably Self Organizing Maps (SOM) and Simple Recurrent Networks (SRN), of which the latter is the precursor to today’s neural language models.\n",
    "\n",
    "Later developments are basically only refinements of these early models. Topic models are refinements of LSA, and include methods like probabilistic LSA (PLSA) and Latent Dirichlet Allocation (LDA).  \n",
    "\n",
    "The main difference between these various models is the type of contextual information they use. LSA and topic models use documents as contexts, which is a legacy from their roots in information retrieval. Neural language models and distributional semantic models instead use words as contexts, which is arguably more natural from a linguistic and cognitive perspective. These different contextual representations capture different types of semantic similarity; the document-based models capture semantic relatedness (e.g. “boat” – “water”) while the word-based models capture semantic similarity (e.g. “boat” – “ship”).  \n",
    "\n",
    "There is no qualitative difference between (current) predictive neural network models and count-based distributional semantics models. Rather, they are different computational means to arrive at the same type of semantic model; several recent papers have demonstrated both theoretically and empirically the correspondence between these different types of models. ([Österlund et al. (2015)](http://aclweb.org/anthology/D/D15/D15-1024.pdf), [Levy and Goldberg (2014)](https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf))\n",
    "\n",
    "A good bet is to use a factorized model – either using explicit factorization of a distributional semantic model (available in e.g. the PyDSM python library or the GloVe implementation), or using a neural network model like those implemented in word2vec – since they produce state of the art results and are robust across a wide range of semantic tasks ([Schnabel et al., 2015](http://aclweb.org/anthology/D/D15/D15-1036.pdf)).\n",
    "\n",
    "Word embeddings are one of the few currently successful applications of unsupervised learning. Their main benefit arguably is that they don't require expensive annotation, but can be derived from large unannotated corpora that are readily available. Pre-trained embeddings can then be used in downstream tasks that use small amounts of labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding models\n",
    "\n",
    "Every feed-forward neural network that takes words from a vocabulary as input and embeds them as vectors into a lower dimensional space, which it then fine-tunes through back-propagation, necessarily yields word embeddings as the weights of the first layer, which is usually referred to as Embedding Layer.\n",
    "\n",
    "We assume the following notational standards: We assume a training corpus containing a sequence of $T$ training words $w_1,w_2,w_3,⋯,w_T$ that belong to a vocabulary $V$ whose size is $|V|$. Our models generally consider a context of $n$ words. We associate every word with an input embedding $v_w$ (the eponymous word embedding in the Embedding Layer) with $d$ dimensions and an output embedding $v′_w$ (another word representation whose role will soon become clearer). We finally optimize an objective function $J_θ$ with regard to our model parameters $θ$ and our model outputs some score $f_θ(x)$ for every input $x$.  \n",
    "\n",
    "### A note in language modelling\n",
    "\n",
    "Language models generally try to compute the probability of a word $w_t$ given its $n−1$ previous words, i.e. $p(wt|w_{t−1},⋯w_{t−n+1})$. By applying the chain rule together with the Markov assumption, we can approximate the probability of a whole sentence or document by the product of the probabilities of each word given its n previous words:\n",
    "\n",
    "$$p(w_1,⋯,w_T)=\\prod_i p(w_i|w_{i−1},⋯,w_{i−n+1})$$\n",
    "\n",
    "In n-gram based language models, we can calculate a word's probability based on the frequencies of its constituent n-grams:  \n",
    "\n",
    "$$p(w_t|w_{t−1},⋯,w_{t−n+1})=\\frac{count(w_{t−n+1},⋯,w_{t−1},w_t)}{count(w_{t−n+1},⋯,w_{t−1})}$$\n",
    "\n",
    "More info about n-grams in these [slides](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf) from Stanford.\n",
    "\n",
    "In neural networks, we achieve the same objective using the well-known softmax layer:  \n",
    "\n",
    "$$p(w_t|w_{t−1},⋯,w_{t−n+1})=\\frac{\\exp(h^⊤v′_{w_t})}{\\sum_{w_i∈V}exp(h^⊤v′_{w_i})}$$\n",
    "\n",
    "The inner product $h^⊤v′_{w_t}$ computes the (unnormalized) log-probability of word $w_t$, which we normalize by the sum of the log-probabilities of all words in $V$. $h$ is the output vector of the penultimate network layer (the hidden layer in the feed-forward network in Figure 1), while $v′_w$ is the output embedding of word $w$, i.e. its representation in the weight matrix of the softmax layer. Note that even though $v′_w$ represents the word $w$, it is learned separately from the input word embedding $v_w$, as the multiplications both vectors are involved in differ ($v_w$ is multiplied with an index vector, $v′_w$ with $h$).\n",
    "\n",
    "<figure text-align=center>\n",
    "  <img src=\"nn_language_model-1.jpg\">\n",
    "</figure>  \n",
    "\n",
    "Figure 1: A neural language model (Bengio et al., 2006)\n",
    "\n",
    "Note that we need to calculate the probability of every word $w$ at the output layer of the neural network. To do this efficiently, we perform a matrix multiplication between $h$ and a weight matrix whose rows consist of $v′_w$ of all words $w$ in $V$. We then feed the resulting vector, which is often referred to as a logit, i.e. the output of a previous layer that is not a probability, with $d=|V|$ into the softmax, while the softmax layer \"squashes\" the vector to a probability distribution over the words in $V$.\n",
    "\n",
    "In the state of the art algorithms the fully connected layer is replaced by an LSTM layer.\n",
    "\n",
    "Using this softmax layer, the model tries to maximize the probability of predicting the correct word at every timestep t. The whole model thus tries to maximize the averaged log probability of the whole corpus:\n",
    "\n",
    "$$J_θ=\\frac{1}{T}\\sum_{t=1}^T log p(w_t|w_{t−1},⋯,w_{t−n+1})$$\n",
    "\n",
    "To sample words from the language model at test time, we can either greedily choose the word with the highest probability $p(w_t|w_{t−1}⋯w_{t−n+1})$ at every time step t or use beam search. We can do this for instance to generate arbitrary text sequences or as part of a sequence prediction task, where an LSTM is used as the decoder.\n",
    "\n",
    "## Word2Vec\n",
    "\n",
    "### Continuous bag-of-words (CBOW)\n",
    "\n",
    "While a language model is only able to look at the past words for its predictions, as it is evaluated on its ability to predict each next word in the corpus, a model that just aims to generate accurate word embeddings does not suffer from this restriction. Mikolov et al. thus use both the n words before and after the target word wt to predict it as depicted in Figure 4. They call this continuous bag-of-words (CBOW), as it uses continuous representations whose order is of no importance.\n",
    "\n",
    "<figure text-align=center>\n",
    "    <img src=\"cbow.png\">\n",
    "</figure>\n",
    "Figure 4: Continuous bag-of-words (Mikolov et al., 2013)\n",
    "\n",
    "The objective function of CBOW in turn is only slightly different than the language model one:\n",
    "\n",
    "$$J_θ=\\frac{1}{T}\\sum_{t=1}^T \\log p(w_t|w_{t−n},⋯,w_{t−1},w_{t+1},⋯,w_{t+n})$$\n",
    "\n",
    "Instead of feeding $n$ previous words into the model, the model receives a window of $n$ words around the target word $w_t$ at each time step $t$.\n",
    "\n",
    "### Skip-gram\n",
    "\n",
    "While CBOW can be seen as a precognitive language model, skip-gram turns the language model objective on its head: Instead of using the surrounding words to predict the centre word as with CBOW, skip-gram uses the centre word to predict the surrounding words as can be seen in Figure 5.\n",
    "\n",
    "<figure text-align=center>\n",
    "    <img src=\"skip-gram.png\">\n",
    "</figure>  \n",
    "Figure 5: Skip-gram (Mikolov et al., 2013)\n",
    "\n",
    "$$J_θ=\\frac{1}{T}\\sum_{t=1}^{T} \\sum_{−n≤j≤n,j≠0} \\log p(w_{t+j}|{w_t})$$\n",
    "\n",
    "This is done using the following softmax as a probability model:\n",
    "\n",
    "$$p(w_{t+j}|w_t)=\\frac{exp(v^⊤_{w_t} v′_{w_{t+j}})}{\\sum_{w_i∈V}exp(v^⊤_{w_t}v′_{w_i})}$$\n",
    "\n",
    "Note that $v_{w_i}$ is the embedding of word i, thus a column of the embedding matrix. $V'$ matrix is the matrix to decode the embedding representation into the logits vector. From the logits vector we can calculate the co-occurrence probability.\n",
    "\n",
    "The notation in Mikolov's paper differs slightly from ours, as they denote the centre word with $w_I$ and the surrounding words with $w_O$. If we replace $w_t$ with $w_I, w_{t+j}$ with $w_O$, and swap the vectors in the inner product due to its commutativity, we arrive at the softmax notation in their paper:\n",
    "\n",
    "$$p(w_O|w_I)=\\frac{\\exp(v′^⊤_{w_O}v_{w_I})}{\\sum^V_{w=1}\\exp(v′^⊤_{w_v}v_{w_I})}$$\n",
    "\n",
    "To train this softmax function is extremely computational expensive. For each training iteration the learning gradient must propagate not only through the desired target word output, but the al other outputs. This is computational expensive and the next Word2Vect papers are related to training optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The softmax\n",
    "\n",
    "Now new are going to take a deeper look at the softmax. For more insight please refer to [this link](http://cs231n.github.io/linear-classify/#softmax-classifier).  \n",
    "In the Softmax classifier, the function mapping $f(x_i;W)=W x_i$ represents the unnormalized log probabilities for each class. The asociated loss is a cross-entropy loss that has the form:\n",
    "\n",
    "$$L_i=-\\log \\left( \\frac{\\exp(f_{y_i})}{\\sum_j \\exp(f_j)} \\right) $$ or equivalently $$L_i=-f_{y_i}+\\log \\sum_j \\exp(f_j) $$\n",
    "\n",
    "where we are using the notation $f_j$ to mean the j-th element of the vector of class scores $f$. As before, the full loss for the dataset is the mean of $L_i$ over all training examples together with a regularization term $R(W)$. The function $f_j(z)=\\frac {\\exp(z_j)}{\\sum_k{\\exp(z_k)}}$ is called the softmax function: It takes a vector of arbitrary real-valued scores (in $z$) and squashes it to a vector of values between zero and one that sum to one. The full cross-entropy loss that involves the softmax function might look scary if you’re seeing it for the first time but it is relatively easy to motivate.\n",
    "\n",
    "### Information theory view. \n",
    "\n",
    "The cross-entropy between a “true” distribution $p$ and an estimated distribution $q$ is defined as:\n",
    "\n",
    "$$H(p,q)=−\\sum_x p(x) \\log q(x)$$\n",
    "\n",
    "The Softmax classifier is hence minimizing the cross-entropy between the estimated class probabilities ($q=\\frac{\\exp(f_{y_i}}{\\sum_j \\exp(f_j)}$ as seen above) and the “true” distribution, which in this interpretation is the distribution where all probability mass is on the correct class (i.e. $p=[0,…1,…,0]$ contains a single $1$ at the $y_i$ -th position.). Moreover, since the cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as $H(p,q)=H(p)+DKL(p||q)$, and the entropy of the delta function $p$ is zero, this is also equivalent to minimizing the KL divergence between the two distributions (a measure of distance). In other words, the cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.\n",
    "\n",
    "### Probabilistic interpretation. \n",
    "\n",
    "Looking at the expression, we see that\n",
    "\n",
    "$$ P(y_i∣x_i;W)=\\frac{\\exp(f_{y_i})}{\\sum_j \\exp (f_j)} $$  \n",
    "\n",
    "can be interpreted as the (normalized) probability assigned to the correct label $y_i$ given the image $x_i$ and parameterized by $W$. To see this, remember that the Softmax classifier interprets the scores inside the output vector $f$ as the unnormalized log probabilities. Exponentiating these quantities therefore gives the (unnormalized) probabilities, and the division performs the normalization so that the probabilities sum to one. In the probabilistic interpretation, we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE). A nice feature of this view is that we can now also interpret the regularization term $R(W)$ in the full loss function as coming from a Gaussian prior over the weight matrix $W$, where instead of MLE we are performing the Maximum a posteriori (MAP) estimation. We mention these interpretations to help your intuitions, but the full details of this derivation are beyond the scope of this class.\n",
    "\n",
    "### Practical issues: Numeric stability\n",
    "\n",
    "When you’re writing code for computing the Softmax function in practice, the intermediate terms $\\exp(f_{y_i})$ and $\\sum_j(f_j)$ may be very large due to the exponentials. Dividing large numbers can be numerically unstable, so it is important to use a normalization trick. Notice that if we multiply the top and bottom of the fraction by a constant $C$ and push it into the sum, we get the following (mathematically equivalent) expression:\n",
    "\n",
    "$$\\frac{\\exp(f_{y_i})}{\\sum_j{\\exp(f_j)}}=\\frac{C\\exp(f_{y_i})}{C\\sum_j{\\exp(f_j)}}=\\frac{\\exp(f_{y_i}+\\log C)}{\\sum_j{\\exp(f_j + \\log C)}}$$  \n",
    "We are free to choose the value of $C$. This will not change any of the results, but we can use this value to improve the numerical stability of the computation. A common choice for $C$ is to set $\\log C=−\\max_j(f_j)$. This simply states that we should shift the values inside the vector $f$ so that the highest value is zero. In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123 456 789]\n",
      "[ 0.  0. nan]\n",
      "[-666 -333    0]\n",
      "[5.75274406e-290 2.39848787e-145 1.00000000e+000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cselmo/.conda/envs/doctorado-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
    "print(f)\n",
    "print(p)\n",
    "# instead: first shift the values of f so that the highest number is 0:\n",
    "f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer\n",
    "print(f)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximating the softmax\n",
    "\n",
    "In this section we are going to take a look at the softmax function. If you want more detail about the softmax, please refer to [this link\n",
    "http://cs231n.github.io/linear-classify/#softmax-classifier\n",
    "\n",
    "We observed that mitigating the complexity of computing the final softmax layer has been one of the main challenges in devising better word embedding models, a commonality with machine translation (MT) ([Jean et al.](http://www.aclweb.org/anthology/P15-1001)) and language modelling ([Jozefowicz et al.](https://arxiv.org/pdf/1602.02410.pdf)).\n",
    "\n",
    "There are several strategies that have been proposed to approximate the softmax. These approaches can be grouped into softmax-based and sampling-based approaches. Softmax-based approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency. Sampling-based approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax.\n",
    "\n",
    "We will explore Noise Contrastive Estimation, a Sampling-based approach to approximate the softmax. Then we will take a look to Negative Sampling. \n",
    "\n",
    "### Noise Contrastive Estimation\n",
    "\n",
    "Noise Contrastive Estimation (NCE) ([Gutmann and Hyvärinen](http://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf)) is proposed by [Mnih and Teh](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf).\n",
    "In this case, we train a model to differentiate the target word from noise. We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive, genuine data from noise samples, as can be seen in Figure 4 below.\n",
    "\n",
    "<img src=\"negative_sampling.png\">\n",
    "\n",
    "Figure 4: Noise Contrastive Estimation ([Stephan Gouws' PhD dissertation](https://pdfs.semanticscholar.org/presentation/f775/cea7cd7f368094a853ea396121950d37915c.pdf))\n",
    "\n",
    "For every word $w_i$ given its context ci of n previous words $w_{t−1},⋯,w_{t−n+1}$ in the training set, we thus generate $k$ noise samples $\\tilde{w}_{ik}$ from a noise distribution $Q$. We can sample from the unigram distribution of the training set. As we need labels to perform our binary classification task, we designate all correct words $w_i$ given their context $c_i$ as true ($y=1$) and all noise samples $\\tilde{w}_{ik}$ as false ($y=0$).  \n",
    "\n",
    "We can now use logistic regression to minimize the negative log-likelihood, i.e. cross-entropy of our training examples against the noise (conversely, we could also maximize the positive log-likelihood as some papers do):\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[ \\log P(y=1|w_i,c_i)+ k \\mathbb{E}_{\\tilde{w}_{ik}\\sim Q} \\left[ \\log P(y=0|\\tilde{w}_{ij},c_i) \\right] \\right]$$\n",
    "\n",
    "Instead of computing the expectation $\\mathbb{E}_{\\tilde{w}_{ik}}∼Q$ of our noise samples, which would still require summing over all words in $V$ to predict the normalised probability of a negative label, we can again take the mean with the Monte Carlo approximation:\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[\\log P(y=1|w_i,c_i) + k \\sum^k_{j=1}\\frac{1}{k} \\log P(y=0|\\tilde{w}_{ij},c_i)\\right]$$\n",
    "\n",
    "Which reduces to:\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[\\log P(y=1|w_i,c_i) +  \\sum^k_{j=1} \\log P(y=0|\\tilde{w}_{ij},c_i)\\right]$$\n",
    "\n",
    "By generating $k$ noise samples for every genuine word wi given its context $c$, we are effectively sampling words from two different distributions: Correct words are sampled from the empirical distribution of the training set $P_{train}$ and depend on their context $c$, whereas noise samples come from the noise distribution $Q$. We can thus represent the probability of sampling either a positive or a noise sample as a mixture of those two distributions, which are weighted based on the number of samples that come from each:\n",
    "\n",
    "$$P(y,w|c)=\\frac{1}{k+1}P_{train}(w|c)+\\frac{k}{k+1}Q(w)$$\n",
    "\n",
    "Given this mixture, we can now calculate the probability that a sample came from the training Ptrain distribution as a conditional probability of y given $w$ and $c$:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{\\frac{1}{k+1}P_{train}(w|c)}{\\frac{1}{k+1}P_{train}(w|c)+\\frac{k}{k+1}Q(w)}$$\n",
    "\n",
    "which can be simplified to:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{P_{train}(w|c)}{P_{train}(w|c)+k Q(w)}$$\n",
    "\n",
    "As we don't know $P_train$ (which is what we would like to calculate), we replace $P_train$ with the probability of our model $P$:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{P(w|c)}{P(w|c)+k Q(w)}$$\n",
    "\n",
    "The probability of predicting a noise sample $(y=0)$ is then simply $P(y=0|w,c)=1−P(y=1|w,c)$. Note that computing $P(w|c)$, i.e. the probability of a word w given its context $c$ is essentially the definition of our softmax:\n",
    "\n",
    "$$P(w|c)=\\frac{\\exp h^T v'_w}{\\sum_{w_i∈V}\\exp(h^⊤ v′_{w_i})}$$\n",
    "\n",
    "For notational brevity and unambiguity, let us designate the denominator of the softmax with $Z(c)$, since the denominator only depends on $h$, which is generated from $c$ (assuming a fixed $V$). The softmax then looks like this:\n",
    "\n",
    "$$P(w|c)=\\frac{\\exp h^T v'_w}{Z(c)}$$\n",
    "\n",
    "Having to compute $P(w|c)$ means that -- again -- we need to compute $Z(c)$, which requires us to sum over the probabilities of all words in $V$. In the case of NCE, there exists a neat trick to circumvent this issue: We can treat the normalisation denominator $Z(c)$ as a parameter that the model can learn. \n",
    "Mnih and Teh (2012) and [Vaswani et al.](http://www.aclweb.org/anthology/D13-1140) actually keep $Z(c)$ fixed at 1, which they report does not affect the model's performance. This assumption has the nice side-effect of reducing the model's parameters, while ensuring that the model self-normalises by not depending on the explicit normalisation in $Z(c)$. Indeed, [Zoph et al.](http://www.aclweb.org/anthology/N16-1145.pdf) find that even when learned, $Z(c)$ is close to 1 and has low variance.\n",
    "\n",
    "If we thus set $Z(c)$ to $1$ in the above softmax equation, we are left with the following probability of word $w$ given a context $c$:\n",
    "\n",
    "$$P(w|c)=exp(h^⊤ v′_w)$$\n",
    "\n",
    "Inserting this term in turn in our logistic regression objective finally yields the full NCE loss:\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[\\log\\frac{\\exp(h^⊤ v′_{w_i})}{\\exp(h^⊤ v′_{w_i})+k Q(w_i)}+\\sum^k_{j=1} \\log(1−\\frac{\\exp(h^⊤ v′_{\\tilde{w}_{ij}})}{exp(h^⊤ v′_{\\tilde{w}_{ij}})+k Q(\\tilde{w}_{ij}))}\\right]$$\n",
    "\n",
    "Note that NCE has nice theoretical guarantees: It can be shown that as we increase the number of noise samples k, the NCE derivative tends towards the gradient of the softmax function. [Mnih and Teh (2012)](https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf) report that 25 noise samples are sufficient to match the performance of the regular softmax, with an expected speed-up factor of about 45. For more information on NCE, Chris Dyer has published some excellent [notes](https://arxiv.org/pdf/1410.8251.pdf).\n",
    "\n",
    "One caveat of NCE is that as typically different noise samples are sampled for every training word w, the noise samples and their gradients cannot be stored in dense matrices, which reduces the benefit of using NCE with GPUs, as it cannot benefit from fast dense matrix multiplications. Jozefowicz et al. (2016) and Zoph et al. (2016) independently propose to share noise samples across all training words in a mini-batch, so that NCE gradients can be computed with dense matrix operations, which are more efficient on GPUs.\n",
    "\n",
    "### Negative Sampling\n",
    "\n",
    "Negative Sampling (NEG), the objective that has been popularised by [Mikolov et al. (2013)](https://arxiv.org/pdf/1301.3781), can be seen as an approximation to NCE. As we have mentioned above, NCE can be shown to approximate the loss of the softmax as the number of samples $k$ increases. NEG simplifies NCE and does away with this guarantee, as the objective of NEG is to learn high-quality word representations rather than achieving low perplexity on a test set, as is the goal in language modelling.\n",
    "\n",
    "NEG also uses a logistic loss function to minimise the negative log-likelihood of words in the training set. Recall that NCE calculated the probability that a word $w$ comes from the empirical training distribution $P_{train}$ given a context $c$ as follows:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{\\exp(h^⊤ v′_w)}{\\exp(h^⊤ v′_w)+kQ(w)}$$\n",
    "\n",
    "The key difference to NCE is that NEG only approximates this probability by making it as easy to compute as possible. For this reason, it sets the most expensive term, kQ(w) to 1, which leaves us with:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{\\exp(h⊤v′w)}{\\exp(h⊤v′w)+1}$$\n",
    "\n",
    "kQ(w)=1 is exactly then true, when $k=|V|$ and $Q$ is a uniform distribution. In this case, NEG is equivalent to NCE. The reason we set $kQ(w)=1$ and not to some other constant can be seen by rewriting the equation, as $P(y=1|w,c)$ can be transformed into the sigmoid function:\n",
    "\n",
    "$$P(y=1|w,c)=\\frac{1}{1+exp(−h^⊤ v′_w)}$$\n",
    "\n",
    "If we now insert this back into the logistic regression loss from before, we get:\n",
    "\n",
    "$$J_θ = −\\sum_{w_i∈V}\\left[ \\log{\\frac{1}{1+\\exp(−h^⊤ v′_{w_i})}} + \\sum^k_{j=1} \\log\\left(1-\\frac{1}{1+\\exp{ (-h^T v'_{w_{ij}}) }}\\right)\\right]$$\n",
    "\n",
    "By simplifying slightly, we obtain:\n",
    "\n",
    "$$J_θ = −\\sum_{w_i∈V}\\left[ \\log{\\frac{1}{1+\\exp(−h^⊤ v′_{w_i})}} + \\sum^k_{j=1} \\log \\frac{1}{1+\\exp{ (h^T v'_{w_{ij}}) }}\\right]$$\n",
    "\n",
    "\n",
    "Setting $σ(x)=\\frac{1}{1+\\exp(−x)} $finally yields the NEG loss:\n",
    "\n",
    "$$J_θ=−\\sum_{w_i∈V}\\left[\\log σ(h^⊤ v′_{w_i})+\\sum^k_{j=1} \\log σ(−h^⊤ v′_{\\tilde{w}_{ij}})\\right]$$\n",
    "\n",
    "To conform with the notation of Mikolov et al. (2013), $h$ must be replaced with $v_{w_I}$, $v′_{w_i}$ with $v′_{w_O}$ and $v_{\\tilde{w}_{ij}}$ with $v′_{w_i}$. Also, in contrast to Mikolov's NEG objective, we a) optimise the objective over the whole corpus, b) minimise negative log-likelihood instead of maximising positive log-likelihood (as mentioned before), and c) have already replaced the expectation $\\mathbb{E}[\\tilde{w}_{ik} \\sim Q]$ with its Monte Carlo approximation. For more insights on the derivation of NEG, have a look at [Goldberg and Levy's notes](http://arxiv.org/abs/1402.3722).\n",
    "\n",
    "We have seen that NEG is only equivalent to NCE when $k=|V|$ and $Q$ is uniform. In all other cases, NEG only approximates NCE, which means that it will not directly optimise the likelihood of correct words, which is key for language modelling. While NEG may thus be useful for learning word embeddings, its lack of asymptotic consistency guarantees makes it inappropriate for language modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec with Gensim\n",
    "\n",
    "More info: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"cat\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.test.utils import datapath\n",
    "sentences = word2vec.Text8Corpus('text8',max_sentence_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "for idx,sentence in enumerate(sentences):\n",
    "    if idx==0:\n",
    "        print(len(sentence))\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, size=100, window=5, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123338263, 170052070)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(sentences, total_examples=model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mansion', 0.6176140308380127),\n",
       " ('chamber', 0.6013445854187012),\n",
       " ('palace', 0.5718334913253784),\n",
       " ('manor', 0.5674700736999512),\n",
       " ('rooming', 0.5523747801780701),\n",
       " ('houses', 0.5498023629188538),\n",
       " ('commons', 0.5482867956161499),\n",
       " ('lords', 0.5352542400360107),\n",
       " ('usher', 0.5346463918685913),\n",
       " ('basement', 0.527206301689148)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar([\"house\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('warship', 0.6990572214126587),\n",
       " ('ship', 0.6840620040893555),\n",
       " ('boats', 0.6776019334793091),\n",
       " ('helicopter', 0.6682183742523193),\n",
       " ('seaplane', 0.6619770526885986),\n",
       " ('truck', 0.6486808061599731),\n",
       " ('canoe', 0.6274817585945129),\n",
       " ('sailing', 0.627424955368042),\n",
       " ('hydrofoil', 0.6255449056625366),\n",
       " ('patrol', 0.621356725692749)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar([\"boat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_scores = model.wv.accuracy(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "print(len(analogy_scores[0][\"correct\"]))\n",
    "print(len(analogy_scores[0][\"w\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec in Keras\n",
    "\n",
    "More info in: http://adventuresinmachinelearning.com/word2vec-keras-tutorial/\n",
    "\n",
    "In this section we are going to implement Word2Vec using text8.  \n",
    "For this task, we must generate the positive and the negative samples.\n",
    "\n",
    "### Positive Samples\n",
    "\n",
    "The positive samples are the samples took from the empirical distribution $P_{train}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import helper\n",
    "helper.get_random(\"articles_tom_\",2010,field=\"body_norm\",value={\"$exists\":1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field \"body_norm\" contains a list of normalized senteces generated from the \"body\" field. Since the body is pre-processed the following steps are:\n",
    "\n",
    "1- Count al the $n$-grams in all the documents from 2010. We are going to use n=5.  \n",
    "2- Keep only the $n$-grams that has a frequency greater than min_tok=100.  \n",
    "3- Save the remaining n-grams in a collectio in mongoDB named \"vocabulary_2010_2010\".\n",
    "\n",
    "The generated count of n-grams is stored in a document which \"\\_id\" is \"2010\". The structure is detailed belown:\n",
    "\n",
    "- \\_id: year\n",
    "- n-grams: number of n-grams to compute\n",
    "- articles: number of articles processed\n",
    "- size: total number of n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(filename)\n",
    "    print(vocabulary[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
      "[5234, 3081, 12, 6, 195, 2, 3134]\n",
      "[[9372, 31], [3532, 216], [2172, 519], [1846, 2], [7037, 1], [8919, 26], [7419, 7303], [341, 26], [38, 3971], [1406, 8521]] [1, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "print(data[:7])\n",
    "\n",
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 2000000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "#dot_product = merge([target, context], mode='dot', dot_axes=1)\n",
    "similarity = dot([target, context], normalize=True, axes=0)\n",
    "dot_product = dot([target, context], normalize=False, axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# create the primary training model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(inputs=[input_target, input_context], outputs=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.6917519569396973\n",
      "Nearest to these: sicily, cologne, read, eleven, enforced, floor, sometimes, benz,\n",
      "Nearest to on: rival, unrest, attribute, contribute, saint, shifting, plots, top,\n",
      "Nearest to nine: australian, qualified, transit, orbits, nodes, slight, suggestion, muscular,\n",
      "Nearest to first: several, wheat, policy, dual, stands, relief, growing, release,\n",
      "Nearest to often: ahead, jeff, especially, warrior, noting, deposits, somehow, collect,\n",
      "Nearest to than: active, engineers, entertainment, moses, wearing, gaining, manifold, magical,\n",
      "Nearest to it: expanded, commit, male, morning, manson, aphrodite, spontaneous, buying,\n",
      "Nearest to have: confucius, magnus, gaming, lawyer, plan, confirmed, ka, flexible,\n",
      "Nearest to its: abundance, theme, outside, discharge, feminist, instrumental, vehicles, personality,\n",
      "Nearest to years: hour, boundary, describe, public, congressional, written, drew, equinox,\n",
      "Nearest to history: spiritual, prevented, arrives, accounting, vita, ahmed, detroit, invested,\n",
      "Nearest to to: ulrich, invention, prosperous, excel, preference, uncertainty, ryan, reads,\n",
      "Nearest to four: similarly, serial, torah, st, considered, dominant, transformation, interpret,\n",
      "Nearest to are: ruby, rider, taught, services, average, shape, physiological, tremendous,\n",
      "Nearest to he: apparatus, encyclop, served, bugs, historians, addiction, platform, consent,\n",
      "Nearest to only: scene, organic, confession, furthermore, genetic, col, similarities, coat,\n",
      "Iteration 10000, loss=0.7095224261283875\n",
      "Iteration 20000, loss=0.6999700665473938\n",
      "Iteration 30000, loss=0.7352024912834167\n",
      "Iteration 40000, loss=0.7102853059768677\n",
      "Iteration 50000, loss=0.6608971953392029\n",
      "Iteration 60000, loss=0.6980677247047424\n",
      "Iteration 70000, loss=0.6953272819519043\n",
      "Iteration 80000, loss=0.575792133808136\n",
      "Iteration 90000, loss=0.6492032408714294\n",
      "Iteration 100000, loss=0.781392514705658\n",
      "Nearest to these: greeks, subspecies, macedonian, presence, segment, measuring, blocks, examined,\n",
      "Nearest to on: improving, jedi, varying, design, seal, culture, inspired, crust,\n",
      "Nearest to nine: six, yahoo, revolution, le, previous, spaces, earth, treasure,\n",
      "Nearest to first: reflection, predicate, abandoned, fossil, mar, persians, multimedia, spending,\n",
      "Nearest to often: ford, pearl, noting, scotland, fur, subtle, deadly, base,\n",
      "Nearest to than: coastline, rated, mutations, astrology, catholic, politicians, ability, portions,\n",
      "Nearest to it: japanese, macedonian, possibly, experience, millions, tactical, next, promote,\n",
      "Nearest to have: heights, rabbi, congressional, protests, bud, help, verses, lying,\n",
      "Nearest to its: anonymous, allied, tolerance, reportedly, edgar, guinness, rica, confirm,\n",
      "Nearest to years: displaced, manson, demonstrate, trees, ford, spelling, settings, anyway,\n",
      "Nearest to history: brian, happen, devastating, federal, bride, exploitation, ambitious, succeed,\n",
      "Nearest to to: the, a, of, in, and, as, at, desire,\n",
      "Nearest to four: accident, famine, rivals, prevalence, order, impression, predecessors, resources,\n",
      "Nearest to are: epic, stayed, a, dietary, dream, infty, consumer, troubles,\n",
      "Nearest to he: wool, initially, cooking, shah, lands, couldn, inaugural, passenger,\n",
      "Nearest to only: editing, adapted, eta, emperors, kinetic, blue, attested, scoring,\n",
      "Iteration 110000, loss=0.5689693093299866\n",
      "Iteration 120000, loss=0.6251784563064575\n",
      "Iteration 130000, loss=0.7500495314598083\n",
      "Iteration 140000, loss=0.5400122404098511\n",
      "Iteration 150000, loss=16.11809539794922\n",
      "Iteration 160000, loss=0.0021697564516216516\n",
      "Iteration 170000, loss=0.5315905213356018\n",
      "Iteration 180000, loss=0.0005851309397257864\n",
      "Iteration 190000, loss=0.35106414556503296\n",
      "Iteration 200000, loss=0.4719960689544678\n",
      "Nearest to these: baptized, enhance, ira, and, chelsea, nature, was, recognise,\n",
      "Nearest to on: of, and, the, a, in, to, from, as,\n",
      "Nearest to nine: munich, writer, recreation, determining, mall, revolution, make, crown,\n",
      "Nearest to first: lives, strategies, developers, offering, turkey, dragon, formations, israelites,\n",
      "Nearest to often: renaissance, particular, reign, favorable, hero, commentators, variation, succeeded,\n",
      "Nearest to than: boot, cooled, breach, records, nfl, councils, norse, stops,\n",
      "Nearest to it: inventors, coup, crater, spaces, referring, you, weaker, little,\n",
      "Nearest to have: are, when, engage, creates, ulrich, or, ta, in,\n",
      "Nearest to its: implement, regulatory, warren, conventional, carolina, anonymous, concepts, early,\n",
      "Nearest to years: philosophy, parade, freight, biography, exported, texas, injection, context,\n",
      "Nearest to history: happen, federal, lighting, consistently, finance, starred, cutting, variants,\n",
      "Nearest to to: the, of, and, a, is, in, for, from,\n",
      "Nearest to four: genesis, profit, congress, graph, around, letter, sold, imports,\n",
      "Nearest to are: a, is, the, in, of, and, to, by,\n",
      "Nearest to he: hellenistic, provided, resolved, metre, arsenal, solids, beers, afterwards,\n",
      "Nearest to only: uncommon, firmly, vatican, plateau, inquiry, lanka, said, done,\n",
      "Iteration 210000, loss=1.1840031147003174\n",
      "Iteration 220000, loss=0.7116475105285645\n",
      "Iteration 230000, loss=0.684666633605957\n",
      "Iteration 240000, loss=0.4721263349056244\n",
      "Iteration 250000, loss=0.5181216597557068\n",
      "Iteration 260000, loss=0.48023200035095215\n",
      "Iteration 270000, loss=1.0687663555145264\n",
      "Iteration 280000, loss=0.415225625038147\n",
      "Iteration 290000, loss=0.4758254587650299\n",
      "Iteration 300000, loss=15.942384719848633\n",
      "Nearest to these: this, of, and, his, was, as, a, is,\n",
      "Nearest to on: of, in, and, a, was, to, by, from,\n",
      "Nearest to nine: zero, by, and, the, s, of, that, or,\n",
      "Nearest to first: ai, lightning, satire, assigned, lp, conflicts, coded, salvation,\n",
      "Nearest to often: copy, military, district, signal, persons, renaissance, variation, launch,\n",
      "Nearest to than: or, of, the, s, is, by, this, from,\n",
      "Nearest to it: the, s, one, are, of, this, a, to,\n",
      "Nearest to have: and, the, s, are, was, in, by, for,\n",
      "Nearest to its: s, he, the, of, zero, which, this, for,\n",
      "Nearest to years: ft, bomb, clause, syria, neither, adds, standards, drum,\n",
      "Nearest to history: removed, technical, believe, flows, mathematical, desirable, nicolas, fort,\n",
      "Nearest to to: the, is, a, s, of, with, and, as,\n",
      "Nearest to four: db, kills, nearly, spanish, breeds, order, plus, u,\n",
      "Nearest to are: the, to, s, as, of, a, with, was,\n",
      "Nearest to he: s, the, is, or, of, in, with, this,\n",
      "Nearest to only: weights, following, responsible, uncommon, suite, consist, cromwell, surprising,\n",
      "Iteration 310000, loss=1.1605714559555054\n",
      "Iteration 320000, loss=0.38511285185813904\n",
      "Iteration 330000, loss=0.3929499685764313\n",
      "Iteration 340000, loss=0.8668106198310852\n",
      "Iteration 350000, loss=0.31481626629829407\n",
      "Iteration 360000, loss=1.192093321833454e-07\n",
      "Iteration 370000, loss=0.36624374985694885\n",
      "Iteration 380000, loss=2.3841855067985307e-07\n",
      "Iteration 390000, loss=0.3087593615055084\n",
      "Iteration 400000, loss=2.2411588361137547e-05\n",
      "Nearest to these: as, the, in, on, was, by, from, and,\n",
      "Nearest to on: was, of, a, his, for, the, as, this,\n",
      "Nearest to nine: zero, on, of, his, two, for, the, are,\n",
      "Nearest to first: ai, coded, prize, yard, video, israelites, views, assigned,\n",
      "Nearest to often: animal, renaissance, obviously, roberts, troubled, armor, colonel, ecology,\n",
      "Nearest to than: by, two, in, as, to, the, of, with,\n",
      "Nearest to it: the, to, as, or, of, this, a, by,\n",
      "Nearest to have: an, and, the, by, s, his, for, of,\n",
      "Nearest to its: the, with, and, is, s, for, of, a,\n",
      "Nearest to years: est, amiga, reactions, canton, september, default, ending, lowercase,\n",
      "Nearest to history: guided, preceded, variants, focused, responsible, actions, latvia, absolute,\n",
      "Nearest to to: as, is, the, for, with, s, are, a,\n",
      "Nearest to four: mikhail, vietnamese, republic, require, pete, ranks, population, endangered,\n",
      "Nearest to are: was, to, s, for, the, on, with, as,\n",
      "Nearest to he: s, with, of, the, that, zero, by, are,\n",
      "Nearest to only: not, he, be, have, other, through, by, zero,\n",
      "Iteration 410000, loss=0.4328470528125763\n",
      "Iteration 420000, loss=1.2254199981689453\n",
      "Iteration 430000, loss=0.40249213576316833\n",
      "Iteration 440000, loss=0.43736177682876587\n",
      "Iteration 450000, loss=0.44196316599845886\n",
      "Iteration 460000, loss=0.42551225423812866\n",
      "Iteration 470000, loss=0.3064069151878357\n",
      "Iteration 480000, loss=0.0032150510232895613\n",
      "Iteration 490000, loss=1.1800369024276733\n",
      "Iteration 500000, loss=0.37915757298469543\n",
      "Nearest to these: in, this, was, of, one, a, as, not,\n",
      "Nearest to on: of, was, the, and, a, this, at, in,\n",
      "Nearest to nine: it, of, his, as, have, and, to, zero,\n",
      "Nearest to first: more, has, zero, it, are, which, one, were,\n",
      "Nearest to often: world, also, but, the, new, other, have, as,\n",
      "Nearest to than: by, is, which, s, for, were, to, it,\n",
      "Nearest to it: the, by, for, or, was, this, a, have,\n",
      "Nearest to have: the, was, it, from, his, for, by, and,\n",
      "Nearest to its: the, a, was, of, as, their, s, this,\n",
      "Nearest to years: bc, passes, existing, intention, understand, january, forward, purple,\n",
      "Nearest to history: intentionally, williams, conscience, except, because, discs, petersburg, western,\n",
      "Nearest to to: was, in, is, the, and, a, by, as,\n",
      "Nearest to four: links, who, all, several, they, i, most, such,\n",
      "Nearest to are: is, the, with, as, was, to, more, by,\n",
      "Nearest to he: that, with, were, as, in, was, which, is,\n",
      "Nearest to only: is, not, of, on, to, for, and, with,\n",
      "Iteration 510000, loss=0.4150902330875397\n",
      "Iteration 520000, loss=0.3955811560153961\n",
      "Iteration 530000, loss=0.4224291145801544\n",
      "Iteration 540000, loss=1.3024113178253174\n",
      "Iteration 550000, loss=1.0863494873046875\n",
      "Iteration 560000, loss=0.3775330185890198\n",
      "Iteration 570000, loss=0.4217554032802582\n",
      "Iteration 580000, loss=1.0745832920074463\n",
      "Iteration 590000, loss=0.35717034339904785\n",
      "Iteration 600000, loss=0.45462921261787415\n",
      "Nearest to these: of, and, in, as, by, on, to, was,\n",
      "Nearest to on: an, the, other, this, to, by, was, also,\n",
      "Nearest to nine: of, and, in, two, this, by, was, are,\n",
      "Nearest to first: zero, for, that, from, as, with, also, a,\n",
      "Nearest to often: can, who, but, through, first, his, with, when,\n",
      "Nearest to than: all, for, many, which, are, s, he, as,\n",
      "Nearest to it: is, which, by, was, an, be, s, on,\n",
      "Nearest to have: this, the, was, which, its, by, three, for,\n",
      "Nearest to its: is, the, was, s, this, which, to, have,\n",
      "Nearest to years: supervision, january, velocity, passes, mph, estate, carriers, separated,\n",
      "Nearest to history: six, american, first, three, after, can, all, new,\n",
      "Nearest to to: this, by, which, was, in, a, and, of,\n",
      "Nearest to four: they, after, her, who, which, be, most, were,\n",
      "Nearest to are: for, this, the, was, or, by, as, is,\n",
      "Nearest to he: is, have, which, that, were, three, for, as,\n",
      "Nearest to only: are, not, five, as, six, on, he, with,\n",
      "Iteration 610000, loss=16.11809539794922\n",
      "Iteration 620000, loss=0.37955814599990845\n",
      "Iteration 630000, loss=0.4290233552455902\n",
      "Iteration 640000, loss=0.00083517114399001\n",
      "Iteration 650000, loss=1.206501841545105\n",
      "Iteration 660000, loss=1.1567749977111816\n",
      "Iteration 670000, loss=1.192093321833454e-07\n",
      "Iteration 680000, loss=0.4430922269821167\n",
      "Iteration 690000, loss=0.39107146859169006\n",
      "Iteration 700000, loss=0.42001959681510925\n",
      "Nearest to these: which, were, or, on, also, this, be, an,\n",
      "Nearest to on: or, which, were, it, by, was, an, this,\n",
      "Nearest to nine: one, five, two, s, was, when, are, an,\n",
      "Nearest to first: these, has, had, also, are, no, more, were,\n",
      "Nearest to often: her, after, can, his, more, first, into, who,\n",
      "Nearest to than: three, their, he, with, as, two, many, this,\n",
      "Nearest to it: it, other, which, or, on, were, that, also,\n",
      "Nearest to have: also, or, on, his, who, which, were, are,\n",
      "Nearest to its: many, has, for, that, by, was, but, this,\n",
      "Nearest to years: preservation, dwarf, pulling, protons, separated, experimental, creates, superman,\n",
      "Nearest to history: new, american, six, her, see, nine, not, three,\n",
      "Nearest to to: this, by, was, for, to, an, in, and,\n",
      "Nearest to four: who, have, are, other, were, no, which, or,\n",
      "Nearest to are: on, or, which, s, were, this, an, be,\n",
      "Nearest to he: that, many, with, not, as, their, this, it,\n",
      "Nearest to only: six, he, between, see, not, five, are, such,\n",
      "Iteration 710000, loss=0.28388041257858276\n",
      "Iteration 720000, loss=1.1725162267684937\n",
      "Iteration 730000, loss=0.3827536702156067\n",
      "Iteration 740000, loss=0.3868235647678375\n",
      "Iteration 750000, loss=0.9698744416236877\n",
      "Iteration 760000, loss=0.8452541828155518\n",
      "Iteration 770000, loss=1.065303921699524\n",
      "Iteration 780000, loss=0.08208934217691422\n",
      "Iteration 790000, loss=0.38573941588401794\n",
      "Iteration 800000, loss=0.4021856486797333\n",
      "Nearest to these: that, these, with, are, from, or, an, this,\n",
      "Nearest to on: one, be, on, are, it, from, or, an,\n",
      "Nearest to nine: nine, were, by, that, when, its, was, with,\n",
      "Nearest to first: more, have, also, zero, who, i, has, be,\n",
      "Nearest to often: who, first, have, also, can, zero, his, on,\n",
      "Nearest to than: all, into, most, or, from, it, are, was,\n",
      "Nearest to it: that, these, with, are, from, or, an, this,\n",
      "Nearest to have: have, zero, his, be, has, on, one, who,\n",
      "Nearest to its: that, these, with, are, from, or, an, this,\n",
      "Nearest to years: steven, jews, cornell, dee, passes, preservation, engaged, roma,\n",
      "Nearest to history: between, some, at, three, five, two, nine, many,\n",
      "Nearest to to: that, these, with, are, from, or, an, this,\n",
      "Nearest to four: on, one, his, be, who, with, are, it,\n",
      "Nearest to are: that, these, with, are, from, or, an, this,\n",
      "Nearest to he: all, which, when, are, it, from, or, an,\n",
      "Nearest to only: however, he, be, on, six, five, they, all,\n",
      "Iteration 810000, loss=0.002489205449819565\n",
      "Iteration 820000, loss=0.4000723659992218\n",
      "Iteration 830000, loss=0.3727886378765106\n",
      "Iteration 840000, loss=1.1263020038604736\n",
      "Iteration 850000, loss=0.3953838646411896\n",
      "Iteration 860000, loss=1.0456478595733643\n",
      "Iteration 870000, loss=1.454363973607542e-05\n",
      "Iteration 880000, loss=0.11736884713172913\n",
      "Iteration 890000, loss=16.11809539794922\n",
      "Iteration 900000, loss=0.39962145686149597\n",
      "Nearest to these: these, one, no, his, be, also, on, from,\n",
      "Nearest to on: these, one, no, his, be, also, on, from,\n",
      "Nearest to nine: such, with, most, are, it, from, or, an,\n",
      "Nearest to first: zero, on, no, his, one, be, more, these,\n",
      "Nearest to often: these, no, more, one, his, be, on, also,\n",
      "Nearest to than: not, with, such, are, it, from, or, an,\n",
      "Nearest to it: such, with, most, are, it, from, or, an,\n",
      "Nearest to have: such, with, most, are, it, from, or, an,\n",
      "Nearest to its: such, with, most, are, it, from, or, an,\n",
      "Nearest to years: trace, partial, protons, colts, factions, sell, scandal, histories,\n",
      "Nearest to history: over, after, some, between, many, new, three, like,\n",
      "Nearest to to: such, with, most, are, it, from, or, an,\n",
      "Nearest to four: more, on, one, some, be, these, no, his,\n",
      "Nearest to are: such, with, most, are, it, from, or, an,\n",
      "Nearest to he: have, has, when, its, with, such, are, it,\n",
      "Nearest to only: six, these, be, on, he, also, more, his,\n",
      "Iteration 910000, loss=1.065132737159729\n",
      "Iteration 920000, loss=1.061425805091858\n",
      "Iteration 930000, loss=0.35332387685775757\n",
      "Iteration 940000, loss=11.388508796691895\n",
      "Iteration 950000, loss=0.027384381741285324\n",
      "Iteration 960000, loss=0.3321799337863922\n",
      "Iteration 970000, loss=0.3830931484699249\n",
      "Iteration 980000, loss=0.3763282597064972\n",
      "Iteration 990000, loss=1.192093321833454e-07\n",
      "Iteration 1000000, loss=2.3841855067985307e-07\n",
      "Nearest to these: this, which, at, he, not, have, were, has,\n",
      "Nearest to on: this, which, at, he, not, have, were, has,\n",
      "Nearest to nine: this, which, at, he, not, have, were, has,\n",
      "Nearest to first: five, be, no, four, six, zero, also, such,\n",
      "Nearest to often: when, an, between, this, which, at, he, not,\n",
      "Nearest to than: than, can, this, which, at, he, over, not,\n",
      "Nearest to it: this, which, at, he, not, have, were, has,\n",
      "Nearest to have: this, which, at, he, not, have, were, has,\n",
      "Nearest to its: this, which, at, he, not, have, were, has,\n",
      "Nearest to years: trace, conflicts, enterprise, folk, difficulty, asbestos, birth, nigeria,\n",
      "Nearest to history: called, about, however, after, in, have, were, has,\n",
      "Nearest to to: this, which, at, he, not, have, were, has,\n",
      "Nearest to four: five, four, no, six, zero, also, can, some,\n",
      "Nearest to are: this, which, at, he, not, have, were, has,\n",
      "Nearest to he: this, which, at, he, not, have, were, has,\n",
      "Nearest to only: also, no, five, be, six, zero, four, some,\n",
      "Iteration 1010000, loss=0.3699541687965393\n",
      "Iteration 1020000, loss=0.4167942404747009\n",
      "Iteration 1030000, loss=0.4845813512802124\n",
      "Iteration 1040000, loss=0.005484992638230324\n",
      "Iteration 1050000, loss=1.042432427406311\n",
      "Iteration 1060000, loss=1.2150287628173828\n",
      "Iteration 1070000, loss=0.395828515291214\n",
      "Iteration 1080000, loss=1.1020861864089966\n",
      "Iteration 1090000, loss=0.0005381957744248211\n",
      "Iteration 1100000, loss=0.004729888867586851\n",
      "Nearest to these: most, more, all, had, some, they, can, first,\n",
      "Nearest to on: most, more, all, had, some, they, can, first,\n",
      "Nearest to nine: most, more, all, had, some, they, can, first,\n",
      "Nearest to first: most, more, all, had, some, they, can, first,\n",
      "Nearest to often: most, more, all, had, some, they, can, first,\n",
      "Nearest to than: most, more, all, had, some, they, can, first,\n",
      "Nearest to it: most, more, all, had, some, they, can, first,\n",
      "Nearest to have: most, more, all, had, some, they, can, first,\n",
      "Nearest to its: most, more, all, had, some, they, can, first,\n",
      "Nearest to years: nigeria, maya, conflicts, difficulty, powder, television, remarks, manipulation,\n",
      "Nearest to history: called, many, such, can, most, more, all, had,\n",
      "Nearest to to: most, more, all, had, some, they, can, first,\n",
      "Nearest to four: most, more, all, had, some, they, can, first,\n",
      "Nearest to are: most, more, all, had, some, they, can, first,\n",
      "Nearest to he: most, more, all, had, some, they, can, first,\n",
      "Nearest to only: american, than, can, most, more, all, had, such,\n",
      "Iteration 1110000, loss=0.7743798494338989\n",
      "Iteration 1120000, loss=1.016908884048462\n",
      "Iteration 1130000, loss=1.7881411622511223e-06\n",
      "Iteration 1140000, loss=0.9429072737693787\n",
      "Iteration 1150000, loss=0.31321823596954346\n",
      "Nearest to often: used, new, who, many, such, him, can, most,\n",
      "Nearest to than: used, new, who, many, such, him, can, most,\n",
      "Nearest to it: used, new, who, many, such, him, can, most,\n",
      "Nearest to have: used, new, who, many, such, him, can, most,\n",
      "Nearest to its: used, new, who, many, such, him, can, most,\n",
      "Nearest to years: sell, nigeria, losing, explaining, fugue, flags, coil, danube,\n",
      "Nearest to history: there, used, new, who, many, such, like, can,\n",
      "Nearest to to: used, new, who, many, such, him, can, most,\n",
      "Nearest to four: used, new, who, many, such, him, can, most,\n",
      "Nearest to are: used, new, who, many, such, him, can, most,\n",
      "Nearest to he: used, new, who, many, such, him, can, most,\n",
      "Nearest to only: used, new, who, many, such, him, can, most,\n",
      "Iteration 1210000, loss=0.349384605884552\n",
      "Iteration 1220000, loss=1.0780153274536133\n",
      "Iteration 1230000, loss=0.38807412981987\n",
      "Iteration 1240000, loss=1.0476195812225342\n",
      "Iteration 1250000, loss=0.27750200033187866\n",
      "Iteration 1260000, loss=0.0004530382575467229\n",
      "Iteration 1270000, loss=0.36324766278266907\n",
      "Iteration 1280000, loss=16.11809539794922\n",
      "Iteration 1290000, loss=1.1817359924316406\n",
      "Iteration 1300000, loss=0.43833106756210327\n",
      "Nearest to these: her, american, into, when, after, there, through, new,\n",
      "Nearest to on: her, american, into, when, after, there, through, new,\n",
      "Nearest to nine: her, american, into, when, after, there, through, new,\n",
      "Nearest to first: her, american, into, when, after, there, through, new,\n",
      "Nearest to often: her, american, into, when, after, there, through, new,\n",
      "Nearest to than: her, american, into, when, after, there, through, new,\n",
      "Nearest to it: her, american, into, when, after, there, through, new,\n",
      "Nearest to have: her, american, into, when, after, there, through, new,\n",
      "Nearest to its: her, american, into, when, after, there, through, new,\n",
      "Nearest to years: coil, nigeria, limiting, delayed, dragons, chemicals, caspian, hebrew,\n",
      "Nearest to history: her, american, into, when, after, there, through, new,\n",
      "Nearest to to: her, american, into, when, after, there, through, new,\n",
      "Nearest to four: her, american, into, when, after, there, through, new,\n",
      "Nearest to are: her, american, into, when, after, there, through, new,\n",
      "Nearest to he: her, american, into, when, after, there, through, new,\n",
      "Nearest to only: her, american, into, when, after, there, through, new,\n",
      "Iteration 1310000, loss=0.9003815650939941\n",
      "Iteration 1320000, loss=2.3841855067985307e-07\n",
      "Iteration 1330000, loss=4.768482540384866e-05\n",
      "Iteration 1340000, loss=0.4128061532974243\n",
      "Iteration 1350000, loss=0.007794459816068411\n",
      "Iteration 1360000, loss=0.41339656710624695\n",
      "Iteration 1370000, loss=0.3725234568119049\n",
      "Iteration 1380000, loss=0.40263548493385315\n",
      "Iteration 1390000, loss=0.3919404447078705\n",
      "Iteration 1400000, loss=0.920357346534729\n",
      "Nearest to these: like, than, may, see, only, these, american, into,\n",
      "Nearest to on: like, than, may, see, only, these, american, into,\n",
      "Nearest to nine: like, than, may, see, only, these, american, into,\n",
      "Nearest to first: like, than, may, see, only, these, american, into,\n",
      "Nearest to often: national, than, may, see, only, these, de, american,\n",
      "Nearest to than: like, than, may, see, only, these, american, into,\n",
      "Nearest to it: like, than, may, see, only, these, american, into,\n",
      "Nearest to have: like, than, may, see, only, these, american, into,\n",
      "Nearest to its: like, than, may, see, only, these, american, into,\n",
      "Nearest to years: ti, bacon, seldom, poe, move, botswana, bigger, dramatically,\n",
      "Nearest to history: like, than, may, see, only, these, american, into,\n",
      "Nearest to to: like, than, may, see, only, these, american, into,\n",
      "Nearest to four: like, than, may, see, only, these, american, into,\n",
      "Nearest to are: like, than, may, see, only, these, american, into,\n",
      "Nearest to he: like, than, may, see, only, these, american, into,\n",
      "Nearest to only: national, than, may, see, only, these, de, american,\n",
      "Iteration 1410000, loss=1.0767619609832764\n",
      "Iteration 1420000, loss=0.3838688135147095\n",
      "Iteration 1430000, loss=1.0977184772491455\n",
      "Iteration 1440000, loss=0.45139268040657043\n",
      "Iteration 1450000, loss=1.1411371231079102\n",
      "Iteration 1460000, loss=1.1342183351516724\n",
      "Iteration 1470000, loss=0.35127386450767517\n",
      "Iteration 1480000, loss=0.10149616003036499\n",
      "Iteration 1490000, loss=0.4072566032409668\n",
      "Iteration 1500000, loss=0.39030054211616516\n",
      "Nearest to these: over, about, between, however, no, d, would, world,\n",
      "Nearest to on: over, about, between, however, no, d, would, world,\n",
      "Nearest to nine: over, about, between, however, no, d, would, world,\n",
      "Nearest to first: over, about, between, however, no, d, would, world,\n",
      "Nearest to often: over, about, between, however, no, d, would, world,\n",
      "Nearest to than: over, about, between, however, no, d, would, world,\n",
      "Nearest to it: over, about, between, however, no, d, would, world,\n",
      "Nearest to have: over, about, between, however, no, d, would, world,\n",
      "Nearest to its: over, about, between, however, no, d, would, world,\n",
      "Nearest to years: years, motorcycles, manage, precipitation, radius, supernatural, montgomery, warsaw,\n",
      "Nearest to history: over, about, between, however, no, d, would, world,\n",
      "Nearest to to: over, about, between, however, no, d, would, world,\n",
      "Nearest to four: over, about, between, however, no, d, would, world,\n",
      "Nearest to are: over, about, between, however, no, d, would, world,\n",
      "Nearest to he: over, about, between, however, no, d, would, world,\n",
      "Nearest to only: over, about, between, however, no, d, would, world,\n",
      "Iteration 1510000, loss=1.4069766998291016\n",
      "Iteration 1520000, loss=1.3799241781234741\n",
      "Iteration 1530000, loss=0.4250006675720215\n",
      "Iteration 1540000, loss=0.07652907818555832\n",
      "Iteration 1550000, loss=0.37393414974212646\n",
      "Iteration 1560000, loss=0.41310402750968933\n",
      "Iteration 1570000, loss=0.3971506655216217\n",
      "Iteration 1580000, loss=3.8870599269866943\n",
      "Iteration 1590000, loss=1.192093321833454e-07\n",
      "Iteration 1600000, loss=0.41017723083496094\n",
      "Nearest to these: often, use, called, if, include, during, over, about,\n",
      "Nearest to on: often, use, called, if, include, during, over, about,\n",
      "Nearest to nine: often, use, called, if, include, during, over, about,\n",
      "Nearest to first: often, use, called, if, include, during, over, about,\n",
      "Nearest to often: often, use, called, if, include, during, over, about,\n",
      "Nearest to than: often, use, called, if, include, during, over, about,\n",
      "Nearest to it: often, use, called, if, include, during, over, about,\n",
      "Nearest to have: often, use, called, if, include, during, over, about,\n",
      "Nearest to its: often, use, called, if, include, during, over, about,\n",
      "Nearest to years: cultural, trace, obscure, chambers, haydn, comprising, traded, keyboards,\n",
      "Nearest to history: often, use, called, if, include, during, over, about,\n",
      "Nearest to to: often, use, called, if, include, during, over, about,\n",
      "Nearest to four: often, use, called, if, include, during, over, about,\n",
      "Nearest to are: often, use, called, if, include, during, over, about,\n",
      "Nearest to he: often, use, called, if, include, during, over, about,\n",
      "Nearest to only: often, use, called, if, include, during, over, about,\n",
      "Iteration 1610000, loss=0.42485639452934265\n",
      "Iteration 1620000, loss=0.9114304780960083\n",
      "Iteration 1630000, loss=6.30756139755249\n",
      "Iteration 1640000, loss=0.3542154133319855\n",
      "Iteration 1650000, loss=0.4233767092227936\n",
      "Iteration 1660000, loss=7.450861448887736e-05\n",
      "Iteration 1670000, loss=0.45206373929977417\n",
      "Iteration 1680000, loss=1.144374132156372\n",
      "Iteration 1690000, loss=1.192093321833454e-07\n",
      "Iteration 1700000, loss=0.44606509804725647\n",
      "Nearest to these: history, so, often, include, use, called, if, during,\n",
      "Nearest to on: history, so, often, include, use, called, if, during,\n",
      "Nearest to nine: history, so, often, include, use, called, if, during,\n",
      "Nearest to first: history, so, often, include, use, called, if, during,\n",
      "Nearest to often: history, so, often, include, use, called, if, during,\n",
      "Nearest to than: history, so, often, include, use, called, if, during,\n",
      "Nearest to it: history, so, often, include, use, called, if, during,\n",
      "Nearest to have: history, so, often, include, use, called, if, during,\n",
      "Nearest to its: history, so, often, include, use, called, if, during,\n",
      "Nearest to years: trace, saul, comprise, palm, acres, insisted, artifacts, tenth,\n",
      "Nearest to history: history, so, often, include, use, called, if, during,\n",
      "Nearest to to: history, so, often, include, use, called, if, during,\n",
      "Nearest to four: history, so, often, include, use, called, if, during,\n",
      "Nearest to are: history, so, often, include, use, called, if, during,\n",
      "Nearest to he: history, so, often, include, use, called, if, during,\n",
      "Nearest to only: history, so, often, include, use, called, if, during,\n",
      "Iteration 1710000, loss=1.192093321833454e-07\n",
      "Iteration 1720000, loss=0.3725689947605133\n",
      "Iteration 1730000, loss=0.21588397026062012\n",
      "Iteration 1740000, loss=0.4224174916744232\n",
      "Iteration 1750000, loss=0.04027225822210312\n",
      "Iteration 1760000, loss=0.9100911617279053\n"
     ]
    }
   ],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()\n",
    "\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 10000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 100000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
